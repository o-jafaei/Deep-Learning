{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ113SSlawcW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "0F377MZGhxIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "O1ExjPV41I04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#قسمت الف"
      ],
      "metadata": {
        "id": "cGISR8H9GmuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, poem_path, threshold):\n",
        "        self.poem_path = poem_path\n",
        "        self.threshold = threshold\n",
        "        self.load_poem()\n",
        "        self.build_vocab()\n",
        "\n",
        "    def load_poem(self):\n",
        "        with open(self.poem_path, 'r', encoding='utf-8') as file:\n",
        "            poem_lines = [line.strip() for line in file.readlines()]\n",
        "\n",
        "        poem_lines = poem_lines[2::2]\n",
        "\n",
        "        if len(poem_lines) % 2 == 1:\n",
        "            poem_lines = poem_lines[:-1]\n",
        "\n",
        "        poem_lines = [f\"{poem_lines[i]} <sep> {poem_lines[i + 1]}\" for i in range(0, len(poem_lines), 2)]\n",
        "\n",
        "        punctuations = string.punctuation + '«»،؛؟'\n",
        "        self.lines = [[word.lower() for word in line.split() if word not in punctuations] for line in poem_lines]\n",
        "\n",
        "\n",
        "    def build_vocab(self):\n",
        "        words = [word for line in self.lines for word in line]\n",
        "        word_counts = Counter(words)\n",
        "        frequent_words = [word for word, count in word_counts.items() if count >= self.threshold]\n",
        "\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(frequent_words, start=1)}\n",
        "\n",
        "        special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>', '<sep>']\n",
        "        for token in special_tokens:\n",
        "            self.word2idx[token] = len(self.word2idx) + 1\n",
        "\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "vocab = Vocabulary('ferdousi.txt', threshold=2)"
      ],
      "metadata": {
        "id": "J1tt4evK2B2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#قسمت ب"
      ],
      "metadata": {
        "id": "Uqbn747KGpsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FerdousiDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, poem_path, vocab):\n",
        "        self.poem_path = poem_path\n",
        "        self.vocab = vocab\n",
        "        self.load_poem()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poem) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.poem[idx], self.poem[idx + 1]\n",
        "\n",
        "    def load_poem(self):\n",
        "        with open(self.poem_path, 'r', encoding='utf-8') as file:\n",
        "            poem_lines = [line.strip() for line in file.readlines()]\n",
        "\n",
        "        poem_lines = poem_lines[2::2]\n",
        "\n",
        "        if len(poem_lines) % 2 == 1:\n",
        "            poem_lines = poem_lines[:-1]\n",
        "\n",
        "        poem_lines = [f\"{poem_lines[i]} <sep> {poem_lines[i + 1]}\" for i in range(0, len(poem_lines), 2)]\n",
        "\n",
        "        poem_lines = [tokenizer(line, return_tensors='pt') for line in poem_lines]\n",
        "\n",
        "        punctuations = string.punctuation + '«»،؛؟'\n",
        "        poem_lines = [\n",
        "            [word for word in line if word not in punctuations and len(word) > 1]\n",
        "            for line in poem_lines\n",
        "        ]\n",
        "        poem_lines = [line for line in poem_lines if len(line) > 0]\n",
        "\n",
        "        max_len = max(len(line) for line in poem_lines)\n",
        "        poem_lines = [line + ['<pad>'] * (max_len - len(line)) for line in poem_lines]\n",
        "\n",
        "        poem_lines = [['<sos>'] + line + ['<eos>'] for line in poem_lines]\n",
        "\n",
        "        self.poem = [\n",
        "            [\n",
        "                self.vocab.word2idx[word] if word in self.vocab.word2idx else self.vocab.word2idx['<unk>']\n",
        "                for word in line\n",
        "            ]\n",
        "            for line in poem_lines\n",
        "        ]\n",
        "\n",
        "        self.poem = torch.tensor(self.poem).long()\n",
        "\n",
        "\n",
        "dataset = FerdousiDataset('ferdousi.txt', vocab)\n",
        "print(dataset)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "print(train_dataset)\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "3ZhrzgHI3BYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FerdousiDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, poem_path, vocab):\n",
        "        self.poem_path = poem_path\n",
        "        self.vocab = vocab\n",
        "        self.load_poem()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poem) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.poem[idx], self.poem[idx + 1]\n",
        "\n",
        "    def load_poem(self):\n",
        "        with open(self.poem_path, 'r', encoding='utf-8') as f:\n",
        "            poem = [line.strip() for line in f.readlines()]\n",
        "        poem = poem[2:]\n",
        "        poem = poem[:-1] if len(poem) % 2 == 1 else poem\n",
        "        poem = [[poem[i], poem[i + 1]] for i in range(0, len(poem), 2)]\n",
        "        poem = [mesra[0] + ' <sep> ' + mesra[1] for mesra in poem]\n",
        "        poem = [word_tokenize(line) for line in poem]\n",
        "        punctuations = string.punctuation + '«»،؛؟'\n",
        "        poem = [[word for word in line if word not in punctuations] for line in poem]\n",
        "        poem = [line for line in poem if len(line) > 0]\n",
        "        poem = [[word for word in line if len(word) > 1] for line in poem]\n",
        "        self.max_len = max([len(line) for line in poem])\n",
        "        poem = [line + ['<pad>'] * (self.max_len - len(line)) for line in poem]\n",
        "        poem = [['<sos>'] + line + ['<eos>'] for line in poem]\n",
        "\n",
        "        # Create word vectors using the vocabulary\n",
        "        self.poem = []\n",
        "        for line in poem:\n",
        "            line_vec = []\n",
        "            for word in line:\n",
        "                if word in self.vocab.word2idx:\n",
        "                    line_vec.append(self.vocab.word2idx[word])\n",
        "                else:\n",
        "                    line_vec.append(self.vocab.word2idx['<unk>'])\n",
        "            self.poem.append(line_vec)\n",
        "        self.poem = torch.tensor(self.poem).long()\n",
        "\n",
        "vocab = Vocabulary('ferdousi.txt', 2)\n",
        "# create a dataset object\n",
        "dataset = FerdousiDataset('ferdousi.txt', vocab)\n",
        "# split the dataset into train and test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "# create a dataloader for train and test\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "u5H1SOe84-fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#قسمت ج"
      ],
      "metadata": {
        "id": "FEsbk7ouIA_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "Pf_6rbwyrwDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "JWKeUh5or8Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "loss = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = loss(outputs[0], labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "RCRc00T2sGdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}