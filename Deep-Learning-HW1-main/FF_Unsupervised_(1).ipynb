{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca62156d",
      "metadata": {
        "id": "ca62156d"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d42fc0",
      "metadata": {
        "id": "16d42fc0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import convolve2d\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import random\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e05eb57",
      "metadata": {
        "id": "9e05eb57"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51638082",
      "metadata": {
        "id": "51638082"
      },
      "outputs": [],
      "source": [
        "batchsize = 1024\n",
        "learning_rate = 0.03\n",
        "learning_rate_lc = 0.03\n",
        "epochs = 100\n",
        "threshold = 3.0\n",
        "image_shape = (1,28,28)\n",
        "image_1d_shape = np.prod(image_shape)\n",
        "layers = [image_1d_shape,500,500,500,500]\n",
        "softmax_layers = [0,1,2,3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224a16dd",
      "metadata": {
        "id": "224a16dd"
      },
      "source": [
        "# Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f560bc1a",
      "metadata": {
        "id": "f560bc1a"
      },
      "outputs": [],
      "source": [
        "train_dataset = MNIST(\"./data/\",download=True, train=True, transform=ToTensor())\n",
        "test_dataset = MNIST(\"./data/\",download=True, train=False, transform=ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, )\n",
        "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479a061c",
      "metadata": {
        "id": "479a061c"
      },
      "outputs": [],
      "source": [
        "def label_to_oh(y):\n",
        "    y = y.numpy().reshape(-1,1)\n",
        "    ohe = OneHotEncoder().fit(np.arange(10).reshape((10,1)))\n",
        "    ohe_y = ohe.transform(y).toarray()\n",
        "    return torch.Tensor(ohe_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21fd5d3",
      "metadata": {
        "id": "c21fd5d3"
      },
      "outputs": [],
      "source": [
        "def show_image(x):\n",
        "    x = x.squeeze()\n",
        "    plt.imshow(x, cmap=\"gray\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d541a79",
      "metadata": {
        "id": "0d541a79"
      },
      "source": [
        "## Creating negative data\n",
        "![Screenshot%20from%202023-05-03%2021-00-50.png](attachment:Screenshot%20from%202023-05-03%2021-00-50.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3c40d6",
      "metadata": {
        "id": "0a3c40d6"
      },
      "source": [
        "### Generating masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902971da",
      "metadata": {
        "id": "902971da"
      },
      "outputs": [],
      "source": [
        "# The method for generating masks for negative data mentioned by Geoffrey Hinton in the article\n",
        "def mask_gen():\n",
        "    random_iter = np.random.randint(5,10)\n",
        "    random_image = np.random.randint(2, size=image_shape).squeeze().astype(np.float32)\n",
        "    blur_filter = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16\n",
        "    for i in range(random_iter):\n",
        "        random_image = convolve2d(random_image, blur_filter, mode='same', boundary='symm')\n",
        "    mask = (random_image > 0.5).astype(np.float32)\n",
        "    return mask\n",
        "\n",
        "# The method for creating masks for negative data that I tried for testing purposes.\n",
        "def mask_gen1():\n",
        "    n = image_1d_shape\n",
        "    arr1 = np.random.normal(loc=0, scale=0.01, size=int(5*n/8))\n",
        "    arr1 = arr1+ abs(0-arr1.min())\n",
        "    arr2 = np.random.normal(loc=1, scale=0.01, size=int(3*n/8))\n",
        "    arr2 = arr2 + abs(1-arr2.max())\n",
        "    arr = np.concatenate([arr1,arr2])\n",
        "    np.random.shuffle(arr)\n",
        "    mask = arr.reshape(image_shape).astype(np.float32)\n",
        "    return mask\n",
        "\n",
        "show_image(mask_gen())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b116a5",
      "metadata": {
        "id": "28b116a5"
      },
      "source": [
        "### Obtaining the hybrid image by applying the mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4778caa",
      "metadata": {
        "id": "f4778caa"
      },
      "outputs": [],
      "source": [
        "def negative_data_gen(batch):\n",
        "    batch = batch[0]\n",
        "    indexes = torch.randperm(batch.shape[0])\n",
        "    x1 = batch\n",
        "    x2 = batch[indexes]\n",
        "    mask = mask_gen()\n",
        "    merged_x1 = x1*mask\n",
        "    merged_x2 = x2*(1-mask)\n",
        "    hybrid_image = merged_x1+merged_x2\n",
        "    return hybrid_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fbcea5",
      "metadata": {
        "id": "34fbcea5"
      },
      "source": [
        "# Defining the FF Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0458702",
      "metadata": {
        "id": "b0458702"
      },
      "outputs": [],
      "source": [
        "class FFLayer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.optimizer = Adam(self.parameters(), lr=learning_rate)\n",
        "        self.threshold = threshold\n",
        "        self.epoch_num = epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos,x_neg):\n",
        "        for i in range(self.epoch_num):\n",
        "            out_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            out_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            loss = torch.log(1+ torch.exp(torch.cat([threshold-out_pos,out_neg-threshold]))).mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66042823",
      "metadata": {
        "id": "66042823"
      },
      "source": [
        "# Defining the FF Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c565c7e",
      "metadata": {
        "id": "4c565c7e"
      },
      "outputs": [],
      "source": [
        "class FFNet(nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(FFLayer(layers[i],layers[i+1]).cuda())\n",
        "\n",
        "    def predict(self,x, softmax_layers):\n",
        "        layers_output = torch.Tensor([]).cuda()\n",
        "        h = x\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i in softmax_layers:\n",
        "                layers_output = torch.cat([layers_output,h],1)\n",
        "\n",
        "        return layers_output\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        out_pos, out_neg = x_pos, x_neg\n",
        "        layer_loop = tqdm_notebook(enumerate(self.layers), leave=False)\n",
        "        for i, layer in layer_loop:\n",
        "            layer_loop.set_description(f\"Training Layer: [{i+1}/{len(self.layers)}]\")\n",
        "            out_pos, out_neg = layer.train(out_pos, out_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04aa1c29",
      "metadata": {
        "id": "04aa1c29"
      },
      "source": [
        "# Training the FF Network\n",
        "### Training the network in order to learn the representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291e52ae",
      "metadata": {
        "id": "291e52ae"
      },
      "outputs": [],
      "source": [
        "model = FFNet(layers)\n",
        "\n",
        "model_train_loop = tqdm_notebook(iter(train_loader),leave=True)\n",
        "for batch in model_train_loop:\n",
        "    x_pos = batch[0]\n",
        "    x_neg = negative_data_gen(batch)\n",
        "    x_pos, x_neg = x_pos.view(-1,image_1d_shape).cuda(), x_neg.view(-1,image_1d_shape).cuda()\n",
        "    model.train(x_pos,x_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db32b1e0",
      "metadata": {
        "id": "db32b1e0"
      },
      "source": [
        "# Defining the Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99841f73",
      "metadata": {
        "id": "99841f73"
      },
      "outputs": [],
      "source": [
        "class LinearClassification(nn.Module):\n",
        "    def __init__(self, input_dimension):\n",
        "        super().__init__()\n",
        "        self.epoch_losses = []\n",
        "        self.linear = torch.nn.Linear(input_dimension, 10).cuda()\n",
        "        self.optimizer = SGD(self.parameters(), lr=learning_rate_lc)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.linear(x)\n",
        "\n",
        "    def predict(self,x):\n",
        "        x = x.view(-1,image_1d_shape).cuda()\n",
        "        h_activity = model.predict(x,softmax_layers)\n",
        "        y_h = self.forward(h_activity)\n",
        "        soft_out = self.softmax(y_h)\n",
        "        return soft_out.argmax()\n",
        "\n",
        "\n",
        "    def train(self, data_loader,epoch_num):\n",
        "        linear_loop = tqdm_notebook(range(epoch_num),total=epoch_num)\n",
        "        for i in linear_loop:\n",
        "            batch_losses = []\n",
        "            for batch in iter(data_loader):\n",
        "                x,y = batch\n",
        "                x = x.view(-1,image_1d_shape).cuda()\n",
        "                y_r = label_to_oh(y).cuda()\n",
        "                h_activity = model.predict(x,softmax_layers)\n",
        "                y_h = self.forward(h_activity)\n",
        "                loss = self.criterion(y_h,y_r)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                batch_losses.append(loss)\n",
        "            self.epoch_losses.append(float(sum(batch_losses)/len(batch_losses)))\n",
        "            linear_loop.set_description(f\"Epoch [{i+1}/{epoch_num}]: \")\n",
        "            linear_loop.set_postfix(loss=self.epoch_losses[i])\n",
        "\n",
        "    def test(self, data_loader):\n",
        "        batch_losses = []\n",
        "        test_loss = 0\n",
        "        for batch in iter(data_loader):\n",
        "            x,y = batch\n",
        "            x = x.view(-1,image_1d_shape).cuda()\n",
        "            y_r = label_to_oh(y).cuda()\n",
        "            h_activity = model.predict(x,softmax_layers)\n",
        "            y_h = self.forward(h_activity)\n",
        "            loss = self.criterion(y_h,y_r)\n",
        "            batch_losses.append(loss)\n",
        "        test_loss = float(sum(batch_losses)/len(batch_losses))\n",
        "        return test_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb83becc",
      "metadata": {
        "id": "bb83becc"
      },
      "outputs": [],
      "source": [
        "def neuron_num(layers,softmax_layers):\n",
        "    num = 0\n",
        "    layers = layers[1:]\n",
        "    for i in softmax_layers:\n",
        "            num += layers[i]\n",
        "    return num"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0afe185",
      "metadata": {
        "id": "f0afe185"
      },
      "source": [
        "# Training the Linear Classifier\n",
        "### Training the linear classifier in order to learn a simple linear transformation for predicting the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82b6a9f",
      "metadata": {
        "scrolled": true,
        "id": "c82b6a9f"
      },
      "outputs": [],
      "source": [
        "linear_model = LinearClassification(neuron_num(layers,softmax_layers))\n",
        "losses = linear_model.train(train_loader,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28356e4",
      "metadata": {
        "id": "b28356e4"
      },
      "outputs": [],
      "source": [
        "linear_model.test(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b2ad21",
      "metadata": {
        "id": "11b2ad21"
      },
      "outputs": [],
      "source": [
        "plt.plot(linear_model.epoch_losses)\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6f02c3",
      "metadata": {
        "id": "fe6f02c3"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(test_loader))\n",
        "n = 4\n",
        "fig, ax1 = plt.subplots(1, n)\n",
        "for i in range(n):\n",
        "    x = batch[0][i]\n",
        "    num=int(linear_model.predict(x))\n",
        "    ax1[i].imshow(x.squeeze(),cmap=\"gray\")\n",
        "    ax1[i].set_title(str(num))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}