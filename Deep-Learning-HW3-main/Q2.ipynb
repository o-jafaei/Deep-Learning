{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks8nP96xGgck"
      },
      "source": [
        "Omid Jafaei 401204268"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im7QZMT2Gmaa"
      },
      "source": [
        "Answer of Theoretical Questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzCozeYGz87"
      },
      "source": [
        "1. In a \"normal\" convolutional network, the sampling locations are fixed and follow a regular grid pattern. On the other hand, in a \"deformable\" convolutional network, the grid sampling locations are adaptively adjusted according to the objects' scale and shape. This adaptive adjustment is achieved by adding 2D offsets to the regular grid sampling locations, enabling free-form deformation of the sampling grid. These offsets are learned from the preceding feature maps, allowing for local, dense, and adaptive transformations.\n",
        "\n",
        "2. By adding 2D offsets to the regular grid sampling locations, deformable convolution enables adaptive adjustment of the sampling locations according to the objects' scale and shape. Also, de-formable RoI pooling adds an offset to each bin position in the regular bin partition of the previous RoI pooling, allowing for adaptive part localization for objects with different shapes. Overall, de-formable networks create flexibility in geometric transformation in images by introducing adaptive and learnable mechanisms for adjusting sampling locations and part localization, enabling the net-work to better capture and model complex spatial transformations in the input data.\n",
        "\n",
        "3. This is due to the fixed geometric structures in their building modules, which restrict their ability to effectively model complex geometric transformations.\n",
        "\n",
        "4. The offsets are obtained by applying additional convolutional layers over the input feature map. These convolutional layers are responsible for learning the adjustments to the regular grid sampling locations based on the preceding feature maps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbPqOazAG9ga"
      },
      "source": [
        "Implementaions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bp2Io9lfuKuE"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/ms_coco\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip train2017.zip -d /content/ms_coco\n",
        "!unzip annotations_trainval2017.zip -d /content/ms_coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgRVyVWFwGsS",
        "outputId": "2a75ee22-f007-4a09-b69d-496f10a99f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "annotations  train2017\n"
          ]
        }
      ],
      "source": [
        "!ls /content/ms_coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDm4frB_QIsY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import pad\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.ops\n",
        "from torch.optim import Adam\n",
        "from torchvision.transforms import Compose, ToTensor, CenterCrop\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "data_root = '/content/ms_coco'\n",
        "image_folder = 'train2017'\n",
        "annotation_file = 'annotations/instances_train2017.json'\n",
        "\n",
        "transform = Compose([Resize((400, 400)), ToTensor()])\n",
        "\n",
        "coco_dataset = CocoDetection(root=f'{data_root}/{image_folder}',\n",
        "                             annFile=f'{data_root}/{annotation_file}',\n",
        "                             transform=transform)\n",
        "\n",
        "def custom_collate(batch):\n",
        "    max_height = max(img.shape[1] for img, _ in batch)\n",
        "    max_width = max(img.shape[2] for img, _ in batch)\n",
        "    resized_batch = [\n",
        "        pad(img, (0, max_width - img.shape[2], 0, max_height - img.shape[1]), value=0)\n",
        "        for img, _ in batch\n",
        "    ]\n",
        "    stacked_images = torch.stack(resized_batch)\n",
        "    targets = [target for _, target in batch]\n",
        "\n",
        "    return stacked_images, targets\n",
        "\n",
        "total_size = len(coco_dataset)\n",
        "train_size = 20000\n",
        "test_size = 2000\n",
        "redundant = total_size - train_size - test_size\n",
        "\n",
        "\n",
        "train_dataset, test_dataset, redundant_dataset = random_split(coco_dataset, [train_size, test_size, redundant])\n",
        "batch_size = 32\n",
        "torch.manual_seed(97)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qnaeVKO7Z5ep"
      },
      "outputs": [],
      "source": [
        "class classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(classifier,self).__init__()\n",
        "\n",
        "    self.block1=nn.Sequential(nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 16),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.3)\n",
        "                              )\n",
        "    self.block2=nn.Sequential(nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 32),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.2)\n",
        "                              )\n",
        "    self.block3=nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 64),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.1)\n",
        "                              )\n",
        "\n",
        "    self.fc = nn.Linear(in_features=self.get_output(), out_features=2)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def get_output(self):\n",
        "    dummy_input = torch.randn([1, 3, 400, 400])\n",
        "    output = self.block1(dummy_input)\n",
        "    output = self.block2(output)\n",
        "    output = self.block3(output)\n",
        "    flatten_output = torch.flatten(output, start_dim = 1)\n",
        "    return flatten_output.shape[1]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = torch.flatten(x, start_dim = 1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S3elpuMKFk1t"
      },
      "outputs": [],
      "source": [
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=3,\n",
        "                 stride=1,\n",
        "                 padding=1,\n",
        "                 dilation=1,\n",
        "                 bias=False):\n",
        "        super(DeformableConv2d, self).__init__()\n",
        "\n",
        "\n",
        "        kernel_size = kernel_size if type(kernel_size) == tuple else (kernel_size, kernel_size)\n",
        "        self.stride = stride if type(stride) == tuple else (stride, stride)\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "\n",
        "        self.offset_conv = nn.Conv2d(in_channels,\n",
        "                                     2 * kernel_size[0] * kernel_size[1],\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     stride=stride,\n",
        "                                     padding=self.padding,\n",
        "                                     dilation=self.dilation,\n",
        "                                     bias=True)\n",
        "\n",
        "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
        "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
        "\n",
        "        self.modulator_conv = nn.Conv2d(in_channels,\n",
        "                                        1 * kernel_size[0] * kernel_size[1],\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        stride=stride,\n",
        "                                        padding=self.padding,\n",
        "                                        dilation=self.dilation,\n",
        "                                        bias=True)\n",
        "\n",
        "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
        "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
        "\n",
        "        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n",
        "                                      out_channels=out_channels,\n",
        "                                      kernel_size=kernel_size,\n",
        "                                      stride=stride,\n",
        "                                      padding=self.padding,\n",
        "                                      dilation=self.dilation,\n",
        "                                      bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
        "        x = torchvision.ops.deform_conv2d(input=x,\n",
        "                                          offset=offset,\n",
        "                                          weight=self.regular_conv.weight,\n",
        "                                          bias=self.regular_conv.bias,\n",
        "                                          padding=self.padding,\n",
        "                                          mask=modulator,\n",
        "                                          stride=self.stride,\n",
        "                                          dilation=self.dilation)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC5501vAFxYF"
      },
      "outputs": [],
      "source": [
        "class classifier_deform(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(classifier_deform,self).__init__()\n",
        "\n",
        "    self.block1=nn.Sequential(DeformableConv2d(in_channels = 3, out_channels = 16, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 16),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.3)\n",
        "                              )\n",
        "    self.block2=nn.Sequential(DeformableConv2d(in_channels = 16, out_channels = 32, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 32),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.2)\n",
        "                              )\n",
        "    self.block3=nn.Sequential(DeformableConv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 2, padding = 0, bias = False),\n",
        "                              nn.BatchNorm2d(num_features = 64),\n",
        "                              nn.MaxPool2d(kernel_size = 5, stride = 2, padding = 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Dropout(0.1)\n",
        "                              )\n",
        "\n",
        "    self.fc = nn.Linear(in_features=self.get_output(), out_features=2)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def get_output(self):\n",
        "    dummy_input = torch.randn([1, 3, 400, 400])\n",
        "    output = self.block1(dummy_input)\n",
        "    output = self.block2(output)\n",
        "    output = self.block3(output)\n",
        "    flatten_output = torch.flatten(output, start_dim = 1)\n",
        "    return flatten_output.shape[1]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = torch.flatten(x, start_dim = 1)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4smup4PvEqRL"
      },
      "outputs": [],
      "source": [
        "def get_person_labels(target_dicts):\n",
        "    batch_size = len(target_dicts)\n",
        "    y=torch.zeros(batch_size)\n",
        "    for i in range(0,batch_size):\n",
        "        for j in range(0, len(target_dicts[i])):\n",
        "            if target_dicts[i][j]['category_id'] == 1:\n",
        "              y[i]=1\n",
        "              break\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaAlMVDEGIlE"
      },
      "source": [
        "#Normal Conv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P1NXtl-J5sg"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "Loss_train=[]\n",
        "acc_list = []\n",
        "err_list = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tic = tim.time()\n",
        "model1=classifier()\n",
        "model1 = model1.to(device)\n",
        "citeration = nn.CrossEntropyLoss()\n",
        "optimizer= Adam(model1.parameters(), lr = 1e-3)\n",
        "for epoch in range(epochs):\n",
        "    batch_loss = []\n",
        "    batch_acc = []\n",
        "    model1.train()\n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch}/{epochs}', unit='batch', leave=False) as epoch_progress:\n",
        "        for batch_idx, (images, targets) in enumerate(epoch_progress):\n",
        "            y = get_person_labels(targets)\n",
        "            x, y = images.to(device), y.to(device)\n",
        "            y_pred = model1(x)\n",
        "            y = y.to(torch.long)\n",
        "            loss = citeration(y_pred.to(device), y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            batch_loss.append(citeration(softmax(y_pred).to(device), y))\n",
        "            _ , preds = torch.max(softmax(y_pred), 1)\n",
        "            correct_counts = preds.eq(y.view_as(preds))\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            batch_acc.append(acc)\n",
        "        Loss_train.append(float(sum(batch_loss)/len(batch_loss)))\n",
        "        acc_list.append(float(sum(batch_acc)/len(batch_acc)))\n",
        "toc = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L5q0mreG4Ad"
      },
      "outputs": [],
      "source": [
        "training_time =  (toc-tic)/3600\n",
        "print('Training Time (min.):', training_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJOPq2IOj3Kp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.plot(err_list, 'o')\n",
        "plt.title('Error on Different Epochs')\n",
        "plt.xticks(range(1, 1+epochs))\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmXu6tKcYU2E"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.plot(acc_list, 'o')\n",
        "plt.title('Accuracy on Different Epochs')\n",
        "plt.xticks(range(1, 1+epochs))\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjOHkSUWj_F"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_acc=[]\n",
        "test_batch_acc=[]\n",
        "for batch_idx, (a1 ,b2 ) in enumerate(test_loader):\n",
        "    b1 = get_person_labels(b2)\n",
        "    yyy = model(a1.to(device))\n",
        "    _ , preds = torch.max(yyy, 1)\n",
        "    b1 = b1.to(device)\n",
        "    correct_counts = preds.eq(b1)\n",
        "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "    test_acc.append(acc)\n",
        "test_acc = float(sum(test_acc)/len(test_acc))\n",
        "print('Test Accuracy: ', test_acc)\n",
        "print('Test Error: ', 1-test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbZOw61xGkQU"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'epochs': epochs,\n",
        "    'model_state_dict': model2.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "    'train_acc': acc_list,\n",
        "    'train_err': err_list,\n",
        "    'test_acc': test_acc,\n",
        "    'test_err': 1-test_acc,\n",
        "    'training_time (min.)': toc-tic\n",
        "}, 'model1.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOnMH8z9GNm_"
      },
      "source": [
        "#Deformable Conv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7bPu5S2GWfl"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "Loss_train=[]\n",
        "acc_list = []\n",
        "err_list = []\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "2=classifier_deform()\n",
        "model2 = model2.to(device)\n",
        "citeration = nn.CrossEntropyLoss()\n",
        "optimizer= Adam(model2.parameters(), lr = 1e-3)\n",
        "for epoch in range(epochs):\n",
        "    batch_loss = []\n",
        "    batch_acc = []\n",
        "    model2.train()\n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch}/{epochs}', unit='batch', leave=False) as epoch_progress:\n",
        "        for batch_idx, (images, targets) in enumerate(epoch_progress):\n",
        "            y = get_person_labels(targets)\n",
        "            x, y = images.to(device), y.to(device)\n",
        "            y_pred = model2(x)\n",
        "            y = y.to(torch.long)\n",
        "            loss = citeration(y_pred.to(device), y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            softmax = nn.Softmax(dim=1)\n",
        "            batch_loss.append(citeration(softmax(y_pred).to(device), y))\n",
        "            _ , preds = torch.max(softmax(y_pred), 1)\n",
        "            correct_counts = preds.eq(y.view_as(preds))\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            batch_acc.append(acc)\n",
        "        Loss_train.append(float(sum(batch_loss)/len(batch_loss)))\n",
        "        acc_list.append(float(sum(batch_acc)/len(batch_acc)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}